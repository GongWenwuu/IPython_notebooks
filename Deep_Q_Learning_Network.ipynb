{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Deep Q-Learning Network with Keras and OpenAI Gym\n",
    "****\n",
    "<p style=\"text-align: right\"><i>Jesus Perez Colino<br>First version: Jan. 2018<br></i></p>\n",
    "\n",
    "\n",
    "## About this notebook: \n",
    "****\n",
    "Notebook prepared by **Jesus Perez Colino** Version 0.2, First Released: 25/01/2018, Alpha\n",
    "\n",
    "- This work is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). This work is offered for free, with the hope that it will be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Reproducibility conditions for this notebook -------------------\n",
      "Sun Dec 01 2019 \n",
      "\n",
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "numpy 1.16.4\n",
      "scipy 1.2.1\n",
      "tensorflow 1.14.0\n",
      "keras 2.2.4\n",
      "gym 0.15.4\n",
      "matplotlib 3.1.0\n",
      "sklearn 0.21.2\n",
      "pandas 0.24.2\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 94 Stepping 3, GenuineIntel\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import watermark\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline\n",
    "\n",
    "print(' Reproducibility conditions for this notebook '.center(85,'-'))\n",
    "%watermark -n -v -m -p numpy,scipy,tensorflow,keras,gym,matplotlib,sklearn,pandas\n",
    "print('-'*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the CartPole OpenAI environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_env = 'CartPole-v0'\n",
    "batch_size = 64\n",
    "n_episodes = 50\n",
    "\n",
    "env = gym.make(name_env)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "output_dir = 'model_output/cartpole/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- OpenAI Enviroment Succesfully Loaded -------------------------------\n",
      "Env. class name: TimeLimit \n",
      "Env. action space: 2\n",
      "Env. observation space: 4\n",
      "Batch size: 64\n",
      "Number of episodes: 50\n",
      "Directory for the model output: model_output/cartpole/\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if isinstance(env, gym.wrappers.time_limit.TimeLimit):\n",
    "    print(' OpenAI Enviroment Succesfully Loaded '.center(100,'-'))\n",
    "    print(f'Env. class name: {env.class_name()} ' )\n",
    "    print(f'Env. action space: {str(env.action_space.n)}' )\n",
    "    print(f'Env. observation space: {env.observation_space.shape[0]}' )\n",
    "    print(f'Batch size: {batch_size}')\n",
    "    print(f'Number of episodes: {n_episodes}' )\n",
    "    print(f'Directory for the model output: {output_dir}')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01187962,  0.04814167, -0.01811793,  0.03977068])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation is a 1D NumPy array composed of 4 floats: they represent the cart's horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from  PIL import  Image,  ImageDraw\n",
    "try:\n",
    "    from pyglet.gl import gl_info\n",
    "    openai_cart_pole_rendering = True   # no problem, let's use OpenAI gym's rendering function\n",
    "except Exception:\n",
    "    openai_cart_pole_rendering = False  # probably no X server available, let's use our own rendering function\n",
    "\n",
    "def render_cart_pole(env, obs):\n",
    "    if openai_cart_pole_rendering:\n",
    "        # use OpenAI gym's rendering function\n",
    "        return env.render(mode=\"rgb_array\")\n",
    "    else:\n",
    "        # rendering for the cart pole environment (in case OpenAI gym can't do it)\n",
    "        img_w = 600\n",
    "        img_h = 400\n",
    "        cart_w = img_w // 12\n",
    "        cart_h = img_h // 15\n",
    "        pole_len = img_h // 3.5\n",
    "        pole_w = img_w // 80 + 1\n",
    "        x_width = 2\n",
    "        max_ang = 0.2\n",
    "        bg_col = (255, 255, 255)\n",
    "        cart_col = 0x000000 # Blue Green Red\n",
    "        pole_col = 0x669acc # Blue Green Red\n",
    "\n",
    "        pos, vel, ang, ang_vel = obs\n",
    "        img = Image.new('RGB', (img_w, img_h), bg_col)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        cart_x = pos * img_w // x_width + img_w // x_width\n",
    "        cart_y = img_h * 95 // 100\n",
    "        top_pole_x = cart_x + pole_len * np.sin(ang)\n",
    "        top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang)\n",
    "        draw.line((0, cart_y, img_w, cart_y), fill=0)\n",
    "        draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) # draw cart\n",
    "        draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) # draw pole\n",
    "        return np.array(img)\n",
    "\n",
    "def plot_cart_pole(env, obs):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    img = render_cart_pole(env, obs)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 599.5, 399.5, -0.5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAADtUlEQVR4nO3c7UnDUBhAYSNdwjl0DOdo1+gadg7XcA7HiP9ENP1Aj0langcKbS6U90c4XNI0wziOdwD83f3SAwDcCkEFiAgqQERQASKCChDZnFl3CwDAT8PUQTtUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWICCpARFABIoIKEBFUgIigAkQEFSAiqAARQQWIbJYeAI55O+wmjz9uX2aeBC5jhwoQEVSAiKACRAQVICKoABFBZbX8ms+1EVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKlfn7bBbegSYJKgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqgAEUEFiAgqQERQASKCChARVICIoAJEBBUgIqjMbhiGi1//+R1QE1SAyGbpAeASr+/bz/fPD4cFJ4Hj7FBZva8xnfoMayGorJp4ck0ElVXb75+WHgEuJqis3vdrpq6hslbDOI6n1k8uwm/MfSvTmXMcfmPyJLZDBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYh4fB+z888lbpUdKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRAQVICKoABFBBYgIKkBEUAEiggoQEVSAiKACRDZn1odZpgC4AXaoABFBBYgIKkBEUAEiggoQEVSAyAfojyJGniXvKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "img = render_cart_pole(env, obs)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Deep Q-Learning (aprox. Q-value using DNN) to define a Learning Agent in an Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
    "        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
    "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
    "        self.epsilon_decay = 0.995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
    "        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n",
    "        self.learning_rate = 0.01 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
    "        self.model = self._build_model() # private method \n",
    "    \n",
    "    def _build_model(self):\n",
    "        # neural net to approximate Q-value function:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu')) # 1st hidden layer; states as input\n",
    "        # model.add(Dense(64, activation='relu')) # 2nd hidden layer\n",
    "        model.add(Dense(self.action_size, activation='linear')) # 2 actions, so 2 output neurons: 0 and 1 (L/R)\n",
    "        model.compile\n",
    "        model.compile(loss= 'mse', # 'categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # list of previous experiences, enabling re-training later\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # epsilon greedy policy: exploration vs. explotation\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # if not acting randomly, predict reward value based on current state\n",
    "        return np.argmax(act_values[0]) # pick the action that will give the highest reward (i.e., go left or right?)\n",
    "\n",
    "    def replay(self, batch_size): # method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n",
    "        for state, action, reward, next_state, done in minibatch: # extract data for each minibatch sample\n",
    "            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n",
    "            if not done: # if not done, then predict future discounted reward\n",
    "                target = (reward + self.gamma * # (target) = reward + (discount rate gamma) * \n",
    "                          np.amax(self.model.predict(next_state)[0])) # (maximum target Q based on future action a')\n",
    "            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size) # initialise agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Agent interact with the Enviroment: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/250, score: 14, e: 1.0\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\envs\\py37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "episode: 1/250, score: 40, e: 1.0\n",
      "episode: 2/250, score: 28, e: 1.0\n",
      "episode: 3/250, score: 31, e: 0.99\n",
      "episode: 4/250, score: 17, e: 0.99\n",
      "episode: 5/250, score: 12, e: 0.99\n",
      "episode: 6/250, score: 13, e: 0.98\n",
      "episode: 7/250, score: 25, e: 0.98\n",
      "episode: 8/250, score: 10, e: 0.97\n",
      "episode: 9/250, score: 12, e: 0.97\n",
      "episode: 10/250, score: 21, e: 0.96\n",
      "episode: 11/250, score: 24, e: 0.96\n",
      "episode: 12/250, score: 12, e: 0.95\n",
      "episode: 13/250, score: 12, e: 0.95\n",
      "episode: 14/250, score: 16, e: 0.94\n",
      "episode: 15/250, score: 25, e: 0.94\n",
      "episode: 16/250, score: 24, e: 0.93\n",
      "episode: 17/250, score: 14, e: 0.93\n",
      "episode: 18/250, score: 11, e: 0.92\n",
      "episode: 19/250, score: 9, e: 0.92\n",
      "episode: 20/250, score: 12, e: 0.91\n",
      "episode: 21/250, score: 16, e: 0.91\n",
      "episode: 22/250, score: 9, e: 0.9\n",
      "episode: 23/250, score: 12, e: 0.9\n",
      "episode: 24/250, score: 9, e: 0.9\n",
      "episode: 25/250, score: 24, e: 0.89\n",
      "episode: 26/250, score: 18, e: 0.89\n",
      "episode: 27/250, score: 51, e: 0.88\n",
      "episode: 28/250, score: 9, e: 0.88\n",
      "episode: 29/250, score: 15, e: 0.87\n",
      "episode: 30/250, score: 25, e: 0.87\n",
      "episode: 31/250, score: 13, e: 0.86\n",
      "episode: 32/250, score: 23, e: 0.86\n",
      "episode: 33/250, score: 73, e: 0.86\n",
      "episode: 34/250, score: 13, e: 0.85\n",
      "episode: 35/250, score: 54, e: 0.85\n",
      "episode: 36/250, score: 20, e: 0.84\n",
      "episode: 37/250, score: 57, e: 0.84\n",
      "episode: 38/250, score: 21, e: 0.83\n",
      "episode: 39/250, score: 13, e: 0.83\n",
      "episode: 40/250, score: 25, e: 0.83\n",
      "episode: 41/250, score: 20, e: 0.82\n",
      "episode: 42/250, score: 24, e: 0.82\n",
      "episode: 43/250, score: 91, e: 0.81\n",
      "episode: 44/250, score: 23, e: 0.81\n",
      "episode: 45/250, score: 8, e: 0.81\n",
      "episode: 46/250, score: 36, e: 0.8\n",
      "episode: 47/250, score: 15, e: 0.8\n",
      "episode: 48/250, score: 46, e: 0.79\n",
      "episode: 49/250, score: 29, e: 0.79\n",
      "episode: 50/250, score: 73, e: 0.79\n",
      "episode: 51/250, score: 15, e: 0.78\n",
      "episode: 52/250, score: 16, e: 0.78\n",
      "episode: 53/250, score: 18, e: 0.77\n",
      "episode: 54/250, score: 30, e: 0.77\n",
      "episode: 55/250, score: 104, e: 0.77\n",
      "episode: 56/250, score: 40, e: 0.76\n",
      "episode: 57/250, score: 57, e: 0.76\n",
      "episode: 58/250, score: 18, e: 0.76\n",
      "episode: 59/250, score: 16, e: 0.75\n",
      "episode: 60/250, score: 27, e: 0.75\n",
      "episode: 61/250, score: 11, e: 0.74\n",
      "episode: 62/250, score: 16, e: 0.74\n",
      "episode: 63/250, score: 62, e: 0.74\n",
      "episode: 64/250, score: 19, e: 0.73\n",
      "episode: 65/250, score: 37, e: 0.73\n",
      "episode: 66/250, score: 16, e: 0.73\n",
      "episode: 67/250, score: 30, e: 0.72\n",
      "episode: 68/250, score: 15, e: 0.72\n",
      "episode: 69/250, score: 39, e: 0.71\n",
      "episode: 70/250, score: 34, e: 0.71\n",
      "episode: 71/250, score: 40, e: 0.71\n",
      "episode: 72/250, score: 18, e: 0.7\n",
      "episode: 73/250, score: 42, e: 0.7\n",
      "episode: 74/250, score: 23, e: 0.7\n",
      "episode: 75/250, score: 45, e: 0.69\n",
      "episode: 76/250, score: 48, e: 0.69\n",
      "episode: 77/250, score: 29, e: 0.69\n",
      "episode: 78/250, score: 46, e: 0.68\n",
      "episode: 79/250, score: 25, e: 0.68\n",
      "episode: 80/250, score: 35, e: 0.68\n",
      "episode: 81/250, score: 33, e: 0.67\n",
      "episode: 82/250, score: 23, e: 0.67\n",
      "episode: 83/250, score: 42, e: 0.67\n",
      "episode: 84/250, score: 28, e: 0.66\n",
      "episode: 85/250, score: 39, e: 0.66\n",
      "episode: 86/250, score: 29, e: 0.66\n",
      "episode: 87/250, score: 21, e: 0.65\n",
      "episode: 88/250, score: 31, e: 0.65\n",
      "episode: 89/250, score: 17, e: 0.65\n",
      "episode: 90/250, score: 56, e: 0.64\n",
      "episode: 91/250, score: 13, e: 0.64\n",
      "episode: 92/250, score: 36, e: 0.64\n",
      "episode: 93/250, score: 10, e: 0.63\n",
      "episode: 94/250, score: 72, e: 0.63\n",
      "episode: 95/250, score: 128, e: 0.63\n",
      "episode: 96/250, score: 14, e: 0.62\n",
      "episode: 97/250, score: 32, e: 0.62\n",
      "episode: 98/250, score: 30, e: 0.62\n",
      "episode: 99/250, score: 12, e: 0.61\n",
      "episode: 100/250, score: 66, e: 0.61\n",
      "episode: 101/250, score: 80, e: 0.61\n",
      "episode: 102/250, score: 51, e: 0.61\n",
      "episode: 103/250, score: 75, e: 0.6\n",
      "episode: 104/250, score: 23, e: 0.6\n",
      "episode: 105/250, score: 104, e: 0.6\n",
      "episode: 106/250, score: 8, e: 0.59\n",
      "episode: 107/250, score: 95, e: 0.59\n",
      "episode: 108/250, score: 129, e: 0.59\n",
      "episode: 109/250, score: 166, e: 0.58\n",
      "episode: 110/250, score: 30, e: 0.58\n",
      "episode: 111/250, score: 87, e: 0.58\n",
      "episode: 112/250, score: 52, e: 0.58\n",
      "episode: 113/250, score: 18, e: 0.57\n",
      "episode: 114/250, score: 71, e: 0.57\n",
      "episode: 115/250, score: 55, e: 0.57\n",
      "episode: 116/250, score: 43, e: 0.56\n",
      "episode: 117/250, score: 137, e: 0.56\n",
      "episode: 118/250, score: 31, e: 0.56\n",
      "episode: 119/250, score: 74, e: 0.56\n",
      "episode: 120/250, score: 16, e: 0.55\n",
      "episode: 121/250, score: 56, e: 0.55\n",
      "episode: 122/250, score: 199, e: 0.55\n",
      "episode: 123/250, score: 45, e: 0.55\n",
      "episode: 124/250, score: 57, e: 0.54\n",
      "episode: 125/250, score: 17, e: 0.54\n",
      "episode: 126/250, score: 50, e: 0.54\n",
      "episode: 127/250, score: 15, e: 0.53\n",
      "episode: 128/250, score: 19, e: 0.53\n",
      "episode: 129/250, score: 109, e: 0.53\n",
      "episode: 130/250, score: 31, e: 0.53\n",
      "episode: 131/250, score: 22, e: 0.52\n",
      "episode: 132/250, score: 61, e: 0.52\n",
      "episode: 133/250, score: 22, e: 0.52\n",
      "episode: 134/250, score: 18, e: 0.52\n",
      "episode: 135/250, score: 33, e: 0.51\n",
      "episode: 136/250, score: 48, e: 0.51\n",
      "episode: 137/250, score: 27, e: 0.51\n",
      "episode: 138/250, score: 69, e: 0.51\n",
      "episode: 139/250, score: 70, e: 0.5\n",
      "episode: 140/250, score: 49, e: 0.5\n",
      "episode: 141/250, score: 121, e: 0.5\n",
      "episode: 142/250, score: 13, e: 0.5\n",
      "episode: 143/250, score: 97, e: 0.49\n",
      "episode: 144/250, score: 36, e: 0.49\n",
      "episode: 145/250, score: 72, e: 0.49\n",
      "episode: 146/250, score: 70, e: 0.49\n",
      "episode: 147/250, score: 90, e: 0.48\n",
      "episode: 148/250, score: 91, e: 0.48\n",
      "episode: 149/250, score: 47, e: 0.48\n",
      "episode: 150/250, score: 97, e: 0.48\n",
      "episode: 151/250, score: 133, e: 0.47\n",
      "episode: 152/250, score: 36, e: 0.47\n",
      "episode: 153/250, score: 79, e: 0.47\n",
      "episode: 154/250, score: 35, e: 0.47\n",
      "episode: 155/250, score: 42, e: 0.46\n",
      "episode: 156/250, score: 65, e: 0.46\n",
      "episode: 157/250, score: 29, e: 0.46\n",
      "episode: 158/250, score: 20, e: 0.46\n",
      "episode: 159/250, score: 116, e: 0.46\n",
      "episode: 160/250, score: 125, e: 0.45\n",
      "episode: 161/250, score: 126, e: 0.45\n",
      "episode: 162/250, score: 42, e: 0.45\n",
      "episode: 163/250, score: 74, e: 0.45\n",
      "episode: 164/250, score: 44, e: 0.44\n",
      "episode: 165/250, score: 77, e: 0.44\n",
      "episode: 166/250, score: 13, e: 0.44\n",
      "episode: 167/250, score: 100, e: 0.44\n",
      "episode: 168/250, score: 41, e: 0.44\n",
      "episode: 169/250, score: 68, e: 0.43\n",
      "episode: 170/250, score: 22, e: 0.43\n",
      "episode: 171/250, score: 49, e: 0.43\n",
      "episode: 172/250, score: 98, e: 0.43\n",
      "episode: 173/250, score: 102, e: 0.42\n",
      "episode: 174/250, score: 121, e: 0.42\n",
      "episode: 175/250, score: 18, e: 0.42\n",
      "episode: 176/250, score: 123, e: 0.42\n",
      "episode: 177/250, score: 52, e: 0.42\n",
      "episode: 178/250, score: 80, e: 0.41\n",
      "episode: 179/250, score: 60, e: 0.41\n",
      "episode: 180/250, score: 49, e: 0.41\n",
      "episode: 181/250, score: 22, e: 0.41\n",
      "episode: 182/250, score: 62, e: 0.41\n",
      "episode: 183/250, score: 180, e: 0.4\n",
      "episode: 184/250, score: 128, e: 0.4\n",
      "episode: 185/250, score: 91, e: 0.4\n",
      "episode: 186/250, score: 108, e: 0.4\n",
      "episode: 187/250, score: 128, e: 0.4\n",
      "episode: 188/250, score: 68, e: 0.39\n",
      "episode: 189/250, score: 104, e: 0.39\n",
      "episode: 190/250, score: 12, e: 0.39\n",
      "episode: 191/250, score: 15, e: 0.39\n",
      "episode: 192/250, score: 14, e: 0.39\n",
      "episode: 193/250, score: 77, e: 0.38\n",
      "episode: 194/250, score: 73, e: 0.38\n",
      "episode: 195/250, score: 79, e: 0.38\n",
      "episode: 196/250, score: 125, e: 0.38\n",
      "episode: 197/250, score: 85, e: 0.38\n",
      "episode: 198/250, score: 94, e: 0.37\n",
      "episode: 199/250, score: 83, e: 0.37\n",
      "episode: 200/250, score: 122, e: 0.37\n",
      "episode: 201/250, score: 74, e: 0.37\n",
      "episode: 202/250, score: 28, e: 0.37\n",
      "episode: 203/250, score: 29, e: 0.37\n",
      "episode: 204/250, score: 74, e: 0.36\n",
      "episode: 205/250, score: 48, e: 0.36\n",
      "episode: 206/250, score: 199, e: 0.36\n",
      "episode: 207/250, score: 36, e: 0.36\n",
      "episode: 208/250, score: 115, e: 0.36\n",
      "episode: 209/250, score: 121, e: 0.35\n",
      "episode: 210/250, score: 78, e: 0.35\n",
      "episode: 211/250, score: 26, e: 0.35\n",
      "episode: 212/250, score: 43, e: 0.35\n",
      "episode: 213/250, score: 12, e: 0.35\n",
      "episode: 214/250, score: 29, e: 0.35\n",
      "episode: 215/250, score: 45, e: 0.34\n",
      "episode: 216/250, score: 72, e: 0.34\n",
      "episode: 217/250, score: 105, e: 0.34\n",
      "episode: 218/250, score: 121, e: 0.34\n",
      "episode: 219/250, score: 45, e: 0.34\n",
      "episode: 220/250, score: 93, e: 0.34\n",
      "episode: 221/250, score: 44, e: 0.33\n",
      "episode: 222/250, score: 97, e: 0.33\n",
      "episode: 223/250, score: 130, e: 0.33\n",
      "episode: 224/250, score: 87, e: 0.33\n",
      "episode: 225/250, score: 66, e: 0.33\n",
      "episode: 226/250, score: 77, e: 0.33\n",
      "episode: 227/250, score: 88, e: 0.32\n",
      "episode: 228/250, score: 76, e: 0.32\n",
      "episode: 229/250, score: 65, e: 0.32\n",
      "episode: 230/250, score: 10, e: 0.32\n",
      "episode: 231/250, score: 110, e: 0.32\n",
      "episode: 232/250, score: 159, e: 0.32\n",
      "episode: 233/250, score: 13, e: 0.31\n",
      "episode: 234/250, score: 50, e: 0.31\n",
      "episode: 235/250, score: 128, e: 0.31\n",
      "episode: 236/250, score: 13, e: 0.31\n",
      "episode: 237/250, score: 28, e: 0.31\n",
      "episode: 238/250, score: 49, e: 0.31\n",
      "episode: 239/250, score: 84, e: 0.3\n",
      "episode: 240/250, score: 16, e: 0.3\n",
      "episode: 241/250, score: 41, e: 0.3\n",
      "episode: 242/250, score: 86, e: 0.3\n",
      "episode: 243/250, score: 71, e: 0.3\n",
      "episode: 244/250, score: 104, e: 0.3\n",
      "episode: 245/250, score: 184, e: 0.3\n",
      "episode: 246/250, score: 72, e: 0.29\n",
      "episode: 247/250, score: 55, e: 0.29\n",
      "episode: 248/250, score: 137, e: 0.29\n",
      "episode: 249/250, score: 95, e: 0.29\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 250\n",
    "done = False\n",
    "for e in range(n_episodes): # iterate over new episodes of the game\n",
    "    state = env.reset() # reset state at start of each new episode of the game\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(5000):  # time represents a frame of the game; goal is to keep pole upright as long as possible up to range, e.g., 500 or 5000 timesteps\n",
    "        # env.render()\n",
    "        action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
    "        next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
    "        reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
    "        state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
    "        if done : # episode ends if agent drops pole or we reach timestep 5000\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
    "                  .format(e, n_episodes, time, agent.epsilon))\n",
    "            break # exit loop\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) + \".hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
